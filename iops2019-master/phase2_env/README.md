# 决赛评估环境

确定进入决赛的队伍之后，首先会为每支队伍分配一定金额的Azure订阅，选手自行启动虚拟机的实例（型号和操作系统没有限制，建议和评测使用的型号**NC12**，操作系统**Ubuntu16.04LTS**保持一致，但是出于某些原因非要选择其他的也不是不可以）进行决赛程序的调试。

调试阶段不放出决赛数据，选手使用预赛数据进行调试。事实上**整个决赛阶段选手都不会接触到决赛数据**，但是可以放心，决赛数据和预赛数据是很相似的。

调试需要做到能够正确通过github上给出的控制程序（具体如何运行下文会给出命令）得到正确的fscore结果，调试时间是一周，具体时间节点请看官网和官方通知群里。

然后我们人工检查（检查Dockerfile，目的是避免潜在的作弊行为）并编译每支队伍的**Docker Image**，通过我们的控制脚本运行选手的程序，在决赛数据集上进行训练和测试，训练和测试的总用时暂定不得超过**24*7小时**。评测时计算资源为每支队伍占用一台**NC12（12 CPU，2 Tesla K80 GPU，112G RAM）**，运行的操作系统是**Ubuntu 16.04LTS**。

# 选手需要准备的内容

- 一个Docker context(一个文件夹)，包含了选手的程序和Dockerfile。
  请确保在Dockerfile中将Context中必要的程序文件拷贝到Docker镜像的合适的位置
- 在给出的Docker镜像中启动训练和测试程序所用的命令。以如下格式存放在Docker context文件夹的"config.json"文件中

```json
{
    "team": ${队伍名，如"railgun"}
    "train": ${训练程序命令，如"python train.py"},
    "test": ${测试程序命令，如"python test.py"},
}
```

请参考`client_example` 

在当前目录运行`test.sh`（这个脚本是在模拟之后的主办方操作流程），并且把Docker Context文件夹放在这里（和`client_example`同级），可以测试是否能够正确运行。

# 主办方操作流程

1. 运行`bootstrap.sh`配置新虚拟机的环境

2. 把选手准备好的Docker Context文件夹拷贝到新虚拟机的某个目录下

3. 检查每个选手的Dockerfile，主要针对可能的作弊行为。

4. 编译Docker镜像，准备每个选手会用到的可写入路径等资源

   ```bash
   python build_env.py -b ${存放context文件夹的目录} -o ${输出配置文件路径}
   ```

   输出文件是一个JSON文件，包含每个队伍的队名，镜像名，操作命令，存储路径等信息。

5. 训练

   ```bash
   python monitor_train.py -c ${配置文件路径} -t ${训练数据路径}
   ```

   训练数据是一个CSV文件，包含了所有KPI的时间戳，值，标注，名字等信息。

6. 测试

```bash
python monitor_test.py -c ${配置文件路径} -g ${Ground Truth路径}
```

测试数据是一个HDF文件，包含了所有KPI的时间戳，标注，名字等信息。

# 训练流程

1. 通过Docker启动选手程序， 包含如下两个命令行参数
   1. 一个**可写入并持久保存**（在测试时可用）的路径，可将训练好的模型文件等存储在这里
   2. 训练数据文件的路径，所有KPI的数据都在这一个文件中。格式是CSV，和预赛阶段的格式相同

# 测试流程

对于每一条KPI:

1. 通过Docker启动选手程序，包含如下两个命令行参数
   1. 一个**只读**的路径（和训练时给出的用于持久保存地址相同）
   2. KPI ID
2. 等待选手程序在标准输出发送一行: "IOPS Phase2 Test Ready"。 请选手注意务必刷新输出缓冲区
3. 向选手程序依序发送一个KPI数据点的时间戳和值。 一行逗号分隔的两个值，时间戳(long int)和KPI值(float): timestamp,value
4. 从选手程序的标准输出获取检测结果。一行，只有一个整数，0或者1。 选手程序需要注意及时刷新输出缓冲区
5. 重复3~4直到这条KPI所有数据点都检测完成。
6. 发送一行：KPI FINISH。选手需要在接收到这一行之后退出测试程序

